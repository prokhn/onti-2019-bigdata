{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gzserver with process ID= 20933\n",
      "Starting gzserver with process ID= 20988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/nti/gymfc/examples/controllers/common/kfac.py:155: UserWarning: volatile was removed (Variable.volatile is always False)\n",
      "  if input[0].volatile == False and self.steps % self.Ts == 0:\n",
      "/home/ec2-user/nti/gymfc/examples/controllers/common/Model.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  act = self.actor_output_act(self.actor_linear(out))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Lapack Error syev : 31 off-diagonal elements didn't converge to zero at /pytorch/aten/src/TH/generic/THTensorLapack.cpp:383",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-eafaad3cd51f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;31m#     else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;31m#         run()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-eafaad3cd51f>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(env_id)\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0macktr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minteract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0macktr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_episodes\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mEPISODES_BEFORE_TRAIN\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m             \u001b[0macktr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0macktr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode_done\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macktr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_episodes\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mEVAL_INTERVAL\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macktr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEVAL_EPISODES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nti/gymfc/examples/controllers/ACKTR.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0;31m# predict softmax action based on state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/nti/gymfc/examples/controllers/common/kfac.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    218\u001b[0m                     self.m_aa[m].cpu().double(), eigenvectors=True)\n\u001b[1;32m    219\u001b[0m                 self.d_g[m], self.Q_g[m] = torch.symeig(\n\u001b[0;32m--> 220\u001b[0;31m                     self.m_gg[m].cpu().double(), eigenvectors=True)\n\u001b[0m\u001b[1;32m    221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_a\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_g\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_g\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_g\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ_g\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Lapack Error syev : 31 off-diagonal elements didn't converge to zero at /pytorch/aten/src/TH/generic/THTensorLapack.cpp:383"
     ]
    }
   ],
   "source": [
    "# nti/experiments_dikower/controllers/madrl/\n",
    "from ACKTR import DisjointACKTR as ACKTR\n",
    "from ACKTR import JointACKTR as ACKTR\n",
    "from common.utils import agg_double_list\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import gym\n",
    "import gymfc\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import nest_asyncio\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "MAX_EPISODES = 5000\n",
    "EPISODES_BEFORE_TRAIN = 0\n",
    "EVAL_EPISODES = 10\n",
    "EVAL_INTERVAL = 100\n",
    "\n",
    "# roll out n steps\n",
    "ROLL_OUT_N_STEPS = 10\n",
    "# only remember the latest ROLL_OUT_N_STEPS\n",
    "MEMORY_CAPACITY = ROLL_OUT_N_STEPS\n",
    "# only use the latest ROLL_OUT_N_STEPS for training A2C\n",
    "BATCH_SIZE = ROLL_OUT_N_STEPS\n",
    "\n",
    "REWARD_DISCOUNTED_GAMMA = 0.99\n",
    "ENTROPY_REG = 0.00\n",
    "#\n",
    "DONE_PENALTY = -10.\n",
    "\n",
    "CRITIC_LOSS = \"mse\"\n",
    "MAX_GRAD_NORM = None\n",
    "\n",
    "EPSILON_START = 0.99\n",
    "EPSILON_END = 0.05\n",
    "EPSILON_DECAY = 500\n",
    "\n",
    "RANDOM_SEED = 2019\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "config_path = os.path.join(current_dir, \"../configs/iris.config\")\n",
    "os.environ[\"GYMFC_CONFIG\"] = config_path\n",
    "\n",
    "class RewScale(gym.RewardWrapper):\n",
    "    def __init__(self, env, scale):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "        self.scale = scale\n",
    "\n",
    "    def reward(self, r):\n",
    "        return r * self.scale\n",
    "\n",
    "\n",
    "def run(env_id=\"AttFC_GyroErr-MotorVel_M4_Con-v0\"):\n",
    "    env = gym.make(env_id)\n",
    "    env = RewScale(env, 0.1)\n",
    "    \n",
    "    env.seed(RANDOM_SEED)\n",
    "    env_eval = gym.make(env_id)\n",
    "    env_eval = RewScale(env_eval, 0.1)\n",
    "    env_eval.seed(RANDOM_SEED)\n",
    "    state_dim = env.observation_space.shape[0]\n",
    "    if len(env.action_space.shape) > 1:\n",
    "        action_dim = env.action_space.shape[0]\n",
    "    else:\n",
    "        action_dim = env.action_space.shape[0]\n",
    "\n",
    "    acktr = ACKTR(env=env, memory_capacity=MEMORY_CAPACITY,\n",
    "              state_dim=state_dim, action_dim=action_dim,\n",
    "              batch_size=BATCH_SIZE, entropy_reg=ENTROPY_REG,\n",
    "              done_penalty=DONE_PENALTY, roll_out_n_steps=ROLL_OUT_N_STEPS,\n",
    "              reward_gamma=REWARD_DISCOUNTED_GAMMA,\n",
    "              epsilon_start=EPSILON_START, epsilon_end=EPSILON_END,\n",
    "              epsilon_decay=EPSILON_DECAY, max_grad_norm=MAX_GRAD_NORM,\n",
    "              episodes_before_train=EPISODES_BEFORE_TRAIN,\n",
    "              critic_loss=CRITIC_LOSS)\n",
    "\n",
    "    episodes =[]\n",
    "    eval_rewards =[]\n",
    "    while acktr.n_episodes < MAX_EPISODES:\n",
    "        acktr.interact()\n",
    "        if acktr.n_episodes >= EPISODES_BEFORE_TRAIN:\n",
    "            acktr.train()\n",
    "        if acktr.episode_done and ((acktr.n_episodes+1)%EVAL_INTERVAL == 0):\n",
    "            rewards, _ = acktr.evaluation(env_eval, EVAL_EPISODES)\n",
    "            rewards_mu, rewards_std = agg_double_list(rewards)\n",
    "            print(\"Episode %d, Average Reward %.2f\" % (acktr.n_episodes+1, rewards_mu))\n",
    "            episodes.append(acktr.n_episodes+1)\n",
    "            eval_rewards.append(rewards_mu)\n",
    "\n",
    "    episodes = np.array(episodes)\n",
    "    eval_rewards = np.array(eval_rewards)\n",
    "    np.savetxt(\"./output/%s_acktr_episodes.txt\"%env_id, episodes)\n",
    "    np.savetxt(\"./output/%s_acktr_eval_rewards.txt\"%env_id, eval_rewards)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(episodes, eval_rewards)\n",
    "    plt.title(\"%s\" % env_id)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Average Reward\")\n",
    "    plt.legend([\"ACKTR\"])\n",
    "    plt.savefig(\"./output/%s_acktr.png\"%env_id)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     if len(sys.argv) >= 2:\n",
    "#         run(sys.argv[1])\n",
    "#     else:\n",
    "#         run()\n",
    "run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
