{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gym\n",
    "import gymfc\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "%matplotlib inline\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "class RewScale(gym.RewardWrapper):\n",
    "    def __init__(self, env, scale):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "        self.scale = scale\n",
    "\n",
    "    def reward(self, r):\n",
    "        return r * self.scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "9f176f783663bb0379c45835e9be42d37641f72e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gzserver with process ID= 3157\n"
     ]
    }
   ],
   "source": [
    "# env = gym.make(\"CartPole-v0\")\n",
    "current_dir = os.getcwd()\n",
    "config_path = os.path.join(current_dir, \"../configs/iris.config\")\n",
    "os.environ[\"GYMFC_CONFIG\"] = config_path\n",
    "env = gym.make('AttFC_GyroErr-MotorVel_M4_Con-v0')\n",
    "env = RewScale(env, 0.1)\n",
    "\n",
    "env.reset()\n",
    "n_actions = env.action_space.shape[0]\n",
    "state_dim = env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "77b9b5d64784ab6c1d031a8f5722b864d20f0df1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ec2-user/.local/lib/python3.5/site-packages/tensorflow/python/ops/nn_grad.py:249: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    }
   ],
   "source": [
    "# create input variables. We only need <s,a,R> for REINFORCE\n",
    "states = tf.placeholder('float32', (None,)+state_dim, name=\"states\")\n",
    "actions = tf.placeholder('int32', name=\"action_ids\")\n",
    "cumulative_rewards = tf.placeholder('float32', name=\"cumulative_returns\")\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.InputLayer(state_dim))\n",
    "model.add(layers.Dense(128, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(n_actions))\n",
    "\n",
    "logits = model(states)\n",
    "policy = tf.nn.softmax(logits)\n",
    "log_policy = tf.nn.log_softmax(logits)\n",
    "\n",
    "indices = tf.stack([tf.range(tf.shape(log_policy)[0]), actions], axis=-1)\n",
    "log_policy_for_actions = tf.gather_nd(log_policy, indices)\n",
    "\n",
    "probabilities = log_policy_for_actions * cumulative_rewards\n",
    "J = tf.reduce_mean(probabilities)\n",
    "\n",
    "# actions_probability = tf.gather_nd(policy[0], actions)\n",
    "entropy = -tf.reduce_sum(policy * tf.log(policy)) / tf.cast(tf.shape(policy)[0], tf.float32)\n",
    "# entropy = -tf.reduce_mean(policy * tf.log(policy))\n",
    "\n",
    "\n",
    "loss = -J - 0.1*entropy\n",
    "update = tf.train.AdamOptimizer().minimize(loss, var_list=model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "da9e20ecddd963e2277f0ccdf745af61c6269dc4"
   },
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, gamma=0.99):\n",
    "    cumulatived_rewards = np.zeros(len(rewards))\n",
    "    local_sum = 0\n",
    "    for i in range(len(rewards) - 1, -1, -1):\n",
    "        local_sum *= gamma\n",
    "        local_sum += rewards[i]\n",
    "        cumulatived_rewards[i] = local_sum \n",
    "\n",
    "    return np.array(cumulatived_rewards)\n",
    "\n",
    "def train_step(_states, _actions, _rewards):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
    "    update.run({states: _states, actions: _actions,\n",
    "                cumulative_rewards: _cumulative_rewards})\n",
    "\n",
    "def get_action_proba(s): \n",
    "    result = policy.eval({states: [s]})[0]\n",
    "    return result\n",
    "\n",
    "def generate_session(t_max=1000):\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "\n",
    "    # arrays to record session\n",
    "    states, actions, rewards = [], [], []\n",
    "    s = env.reset()\n",
    "    for t in range(t_max):\n",
    "        # action probabilities array aka pi(a|s)\n",
    "        action_probas = get_action_proba(s)\n",
    "        a = np.random.choice([0, 1], 1, p=action_probas)[0]\n",
    "        new_s, r, done, info = env.step(a)\n",
    "        # record session history to train later\n",
    "        states.append(s)\n",
    "        actions.append(a)\n",
    "        rewards.append(r)\n",
    "\n",
    "        s = new_s\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    train_step(states, actions, rewards)\n",
    "    return sum(rewards)\n",
    "\n",
    "def test_policy():\n",
    "    actuals = []\n",
    "    desireds = []\n",
    "    env = gym.make()\n",
    "    ob = env.reset()\n",
    "    while True:\n",
    "        desired = env.omega_target\n",
    "        actual = env.omega_actual\n",
    "        actuals.append(actual)\n",
    "        desireds.append(desired)\n",
    "        print(\"sp=\", desired, \" rate=\", actual)\n",
    "        action_probas = get_action_proba(ob)\n",
    "        action = np.random.choice([1, 1, 1, 1], 1, p=action_probas)[0]\n",
    "        ob, _, done, _ = env.step(action)\n",
    "        if done:\n",
    "            break\n",
    "    plot_step_response(np.array(desireds), np.array(actuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "03ab463e49c071af54b4b26bc4ce793ab8e3b26a"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'a' and 'p' must have same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-970ee3030aea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-970ee3030aea>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mgenerate_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# generate new sessions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-0c52c6bdb7ed>\u001b[0m in \u001b[0;36mgenerate_session\u001b[0;34m(t_max)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# action probabilities array aka pi(a|s)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0maction_probas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_action_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maction_probas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mnew_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# record session history to train later\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mmtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: 'a' and 'p' must have same size"
     ]
    }
   ],
   "source": [
    "import time\n",
    "s = tf.InteractiveSession()\n",
    "s.run(tf.global_variables_initializer())\n",
    "\n",
    "start = time.time()\n",
    "for i in range(10000):\n",
    "    \n",
    "    rewards = [generate_session() for _ in range(100)]  # generate new sessions\n",
    "\n",
    "    print('Epoch: {}'.format(i))\n",
    "    print(\"Time: {:.2}s\".format(time.time() - start))\n",
    "    print(\"Mean reward: %.3f\" % (np.mean(rewards)))\n",
    "    print(\"=====================================\")\n",
    "    if np.mean(rewards) > 19:\n",
    "        print(\"You Win!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '__file__' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d469145e2287>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-d469145e2287>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'AttFC_GyroErr-MotorVel_M4_Con-v0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mseeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m239\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mcurrent_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirname\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__file__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     config_path = os.path.join(current_dir,\n\u001b[1;32m     18\u001b[0m                                \"../configs/iris.config\")\n",
      "\u001b[0;31mNameError\u001b[0m: name '__file__' is not defined"
     ]
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_step_response(desired, actual,\n",
    "                       end=1., title=None,\n",
    "                       step_size=0.001, threshold_percent=0.1):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            threshold (float): Percent of the start error\n",
    "    \"\"\"\n",
    "\n",
    "    # actual = actual[:,:end,:]\n",
    "    end_time = len(desired) * step_size\n",
    "    t = np.arange(0, end_time, step_size)\n",
    "\n",
    "    # desired = desired[:end]\n",
    "    threshold = threshold_percent * desired\n",
    "\n",
    "    plot_min = -math.radians(350)\n",
    "    plot_max = math.radians(350)\n",
    "\n",
    "    subplot_index = 3\n",
    "    num_subplots = 3\n",
    "\n",
    "    f, ax = plt.subplots(num_subplots, sharex=True, sharey=False)\n",
    "    f.set_size_inches(10, 5)\n",
    "    if title:\n",
    "        plt.suptitle(title)\n",
    "    ax[0].set_xlim([0, end_time])\n",
    "    res_linewidth = 2\n",
    "    linestyles = [\"c\", \"m\", \"b\", \"g\"]\n",
    "    reflinestyle = \"k--\"\n",
    "    error_linestyle = \"r--\"\n",
    "\n",
    "    # Always\n",
    "    ax[0].set_ylabel(\"Roll (rad/s)\")\n",
    "    ax[1].set_ylabel(\"Pitch (rad/s)\")\n",
    "    ax[2].set_ylabel(\"Yaw (rad/s)\")\n",
    "\n",
    "    ax[-1].set_xlabel(\"Time (s)\")\n",
    "\n",
    "    \"\"\" ROLL \"\"\"\n",
    "    # Highlight the starting x axis\n",
    "    ax[0].axhline(0, color=\"#AAAAAA\")\n",
    "    ax[0].plot(t, desired[:, 0], reflinestyle)\n",
    "    ax[0].plot(t, desired[:, 0] - threshold[:, 0], error_linestyle, alpha=0.5)\n",
    "    ax[0].plot(t, desired[:, 0] + threshold[:, 0], error_linestyle, alpha=0.5)\n",
    "\n",
    "    r = actual[:, 0]\n",
    "    ax[0].plot(t[:len(r)], r, linewidth=res_linewidth)\n",
    "\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    \"\"\" PITCH \"\"\"\n",
    "\n",
    "    ax[1].axhline(0, color=\"#AAAAAA\")\n",
    "    ax[1].plot(t, desired[:, 1], reflinestyle)\n",
    "    ax[1].plot(t, desired[:, 1] - threshold[:, 1], error_linestyle, alpha=0.5)\n",
    "    ax[1].plot(t, desired[:, 1] + threshold[:, 1], error_linestyle, alpha=0.5)\n",
    "    p = actual[:, 1]\n",
    "    ax[1].plot(t[:len(p)], p, linewidth=res_linewidth)\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    \"\"\" YAW \"\"\"\n",
    "    ax[2].axhline(0, color=\"#AAAAAA\")\n",
    "    ax[2].plot(t, desired[:, 2], reflinestyle)\n",
    "    ax[2].plot(t, desired[:, 2] - threshold[:, 2], error_linestyle, alpha=0.5)\n",
    "    ax[2].plot(t, desired[:, 2] + threshold[:, 2], error_linestyle, alpha=0.5)\n",
    "    y = actual[:, 2]\n",
    "    ax[2].plot(t[:len(y)], y, linewidth=res_linewidth)\n",
    "    ax[2].grid(True)\n",
    "\n",
    "    plt.savefig(\"gymfc-ppo-step-response.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RewScale' object has no attribute 'omega_target'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-3fc2d9bbe618>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-44-8dae82c57328>\u001b[0m in \u001b[0;36mtest_policy\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdesireds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mdesired\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0momega_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m         \u001b[0mactual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0momega_actual\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mactuals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'RewScale' object has no attribute 'omega_target'"
     ]
    }
   ],
   "source": [
    "test_policy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
