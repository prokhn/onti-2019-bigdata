{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting gzserver with process ID= 8722\n",
      "episode: 0 \t running_reward: -13.7831 \t reward: -14.0\n",
      "episode: 1 \t running_reward: -18.5629 \t reward: -62.0\n",
      "episode: 2 \t running_reward: -20.8329 \t reward: -41.0\n",
      "Ctrl+C detected, shutting down gazebo and application\n",
      "\n",
      "Simulation Stats\n",
      "-----------------\n",
      "packets_dropped        0\n",
      "time_start_seconds     1553238021.5979114\n",
      "time_lapse_hours       0.006391313009791904\n",
      "steps                  3984\n",
      "\n",
      "\n",
      "Killing process with ID= 8722\n",
      "Timeout connecting to Gazebo\n",
      "\n",
      "Simulation Stats\n",
      "-----------------\n",
      "packets_dropped        0\n",
      "time_start_seconds     1553238021.5979114\n",
      "time_lapse_hours       0.01168374670876397\n",
      "steps                  3984\n",
      "\n",
      "\n",
      "Killing process with ID= 8722\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "Timeout, could not connect to Gazebo",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m Timeout, could not connect to Gazebo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/.local/lib/python3.5/site-packages/IPython/core/interactiveshell.py:3299: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Actor-Critic with continuous action using TD-error as the Advantage, Reinforcement Learning.\n",
    "The Pendulum example (based on https://github.com/dennybritz/reinforcement-learning/blob/master/PolicyGradient/Continuous%20MountainCar%20Actor%20Critic%20Solution.ipynb)\n",
    "Cannot converge!!! oscillate!!!\n",
    "View more on my tutorial page: https://morvanzhou.github.io/tutorials/\n",
    "Using:\n",
    "tensorflow r1.3\n",
    "gym 0.8.0\n",
    "\"\"\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import gymfc\n",
    "import os\n",
    "import random\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  # reproducible\n",
    "\n",
    "\n",
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, action_bound, lr=0.0001):\n",
    "        self.sess = sess\n",
    "\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        self.a = tf.placeholder(tf.float32, None, name=\"act\")\n",
    "        self.td_error = tf.placeholder(tf.float32, None, name=\"td_error\")  # TD_error\n",
    "\n",
    "        l1 = tf.layers.dense(\n",
    "            inputs=self.s,\n",
    "            units=30,  # number of hidden units\n",
    "            activation=tf.nn.relu,\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "#             name='l1'\n",
    "        )\n",
    "\n",
    "        mu = tf.layers.dense(\n",
    "            inputs=l1,\n",
    "            units=1,  # number of hidden units\n",
    "            activation=tf.nn.tanh,\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "#             name='mu'\n",
    "        )\n",
    "\n",
    "        sigma = tf.layers.dense(\n",
    "            inputs=l1,\n",
    "            units=1,  # output units\n",
    "            activation=tf.nn.softplus,  # get action probabilities\n",
    "            kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "            bias_initializer=tf.constant_initializer(1.),  # biases\n",
    "#             name='sigma'\n",
    "        )\n",
    "        global_step = tf.Variable(0, trainable=False)\n",
    "        # self.e = epsilon = tf.train.exponential_decay(2., global_step, 1000, 0.9)\n",
    "        self.mu, self.sigma = tf.squeeze(mu*2), tf.squeeze(sigma+0.1)\n",
    "        self.normal_dist = tf.distributions.Normal(self.mu, self.sigma)\n",
    "#         self.normal_dist.sample(1), action_bound[0], action_bound[1]\n",
    "        self.action = tf.clip_by_value(self.normal_dist.sample(4), action_bound[0], action_bound[1])\n",
    "\n",
    "        with tf.name_scope('exp_v'):\n",
    "            log_prob = self.normal_dist.log_prob(self.a)  # loss without advantage\n",
    "            self.exp_v = log_prob * self.td_error  # advantage (TD_error) guided loss\n",
    "            # Add cross entropy cost to encourage exploration\n",
    "            self.exp_v += 0.01*self.normal_dist.entropy()\n",
    "\n",
    "        with tf.name_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v, global_step)    # min(v) = max(-v)\n",
    "\n",
    "    def learn(self, s, a, td):\n",
    "        s = s[np.newaxis, :]\n",
    "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
    "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        return self.sess.run(self.action, {self.s: s})  # get probabilities for all actions\n",
    "\n",
    "\n",
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, lr=0.01):\n",
    "        self.sess = sess\n",
    "        with tf.name_scope('inputs'):\n",
    "            self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "            self.v_ = tf.placeholder(tf.float32, [1, 1], name=\"v_next\")\n",
    "            self.r = tf.placeholder(tf.float32, name='r')\n",
    "\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=30,  # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "#                 name='l1'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "#                 name='V'\n",
    "            )\n",
    "\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = tf.reduce_mean(self.r + GAMMA * self.v_ - self.v)\n",
    "            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    def learn(self, s, r, s_):\n",
    "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
    "\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error\n",
    "\n",
    "\n",
    "OUTPUT_GRAPH = False\n",
    "MAX_EPISODE = 20000\n",
    "MAX_EP_STEPS = 1000\n",
    "DISPLAY_REWARD_THRESHOLD = -100  # renders environment if total episode reward is greater then this threshold\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9\n",
    "LR_A = 0.5    # learning rate for actor\n",
    "LR_C = 0.09     # learning rate for critic\n",
    "\n",
    "class RewScale(gym.RewardWrapper):\n",
    "    def __init__(self, env, scale):\n",
    "        gym.RewardWrapper.__init__(self, env)\n",
    "        self.scale = scale\n",
    "\n",
    "    def reward(self, r):\n",
    "        return r * self.scale\n",
    "\n",
    "# env = gym.make('Pendulum-v0')\n",
    "current_dir = os.getcwd()\n",
    "config_path = os.path.join(current_dir, \"../configs/iris.config\")\n",
    "os.environ[\"GYMFC_CONFIG\"] = config_path\n",
    "env = gym.make('AttFC_GyroErr-MotorVel_M4_Con-v0')\n",
    "# env = RewScale(env, 0.1)\n",
    "\n",
    "env.seed(1)  # reproducible\n",
    "# env = env.unwrapped\n",
    "\n",
    "N_S = env.observation_space.shape[0]\n",
    "A_BOUND = env.action_space.high\n",
    "\n",
    "sess = tf.Session()\n",
    "\n",
    "actor = Actor(sess, n_features=N_S, lr=LR_A, action_bound=[-A_BOUND, A_BOUND])\n",
    "critic = Critic(sess, n_features=N_S, lr=LR_C)\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "actuals = []\n",
    "desireds = []\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    ep_rs = []\n",
    "    while True:\n",
    "        # if RENDER:\n",
    "#         env.render()\n",
    "        actuals.append(env.omega_actual)\n",
    "        desireds.append(env.omega_target)\n",
    "    \n",
    "        a = actor.choose_action(s)\n",
    "        s_, r, done, info = env.step(a)\n",
    "        r /= 10\n",
    "        \n",
    "        td_error = critic.learn(s, r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        actor.learn(s, a, td_error)  # true_gradient = grad[logPi(s,a) * td_error]\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "        ep_rs.append(r)\n",
    "        if t > MAX_EP_STEPS or done:\n",
    "            ep_rs_sum = sum(ep_rs)\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.9 + ep_rs_sum * 0.1\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"\\t running_reward:\", round(running_reward, 4), \"\\t reward:\", round(ep_rs_sum))\n",
    "            break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "%matplotlib inline\n",
    "def plot_step_response(desired, actual,\n",
    "                       end=1., title=None,\n",
    "                       step_size=0.001, threshold_percent=0.1):\n",
    "    \"\"\"\n",
    "        Args:\n",
    "            threshold (float): Percent of the start error\n",
    "    \"\"\"\n",
    "\n",
    "    # actual = actual[:,:end,:]\n",
    "    end_time = len(desired) * step_size\n",
    "    t = np.arange(0, end_time, step_size)\n",
    "\n",
    "    # desired = desired[:end]\n",
    "    threshold = threshold_percent * desired\n",
    "\n",
    "    plot_min = -math.radians(350)\n",
    "    plot_max = math.radians(350)\n",
    "\n",
    "    subplot_index = 3\n",
    "    num_subplots = 3\n",
    "\n",
    "    f, ax = plt.subplots(num_subplots, sharex=True, sharey=False)\n",
    "    f.set_size_inches(10, 5)\n",
    "    if title:\n",
    "        plt.suptitle(title)\n",
    "    ax[0].set_xlim([0, end_time])\n",
    "    res_linewidth = 2\n",
    "    linestyles = [\"c\", \"m\", \"b\", \"g\"]\n",
    "    reflinestyle = \"k--\"\n",
    "    error_linestyle = \"r--\"\n",
    "\n",
    "    # Always\n",
    "    ax[0].set_ylabel(\"Roll (rad/s)\")\n",
    "    ax[1].set_ylabel(\"Pitch (rad/s)\")\n",
    "    ax[2].set_ylabel(\"Yaw (rad/s)\")\n",
    "\n",
    "    ax[-1].set_xlabel(\"Time (s)\")\n",
    "\n",
    "    \"\"\" ROLL \"\"\"\n",
    "    # Highlight the starting x axis\n",
    "    ax[0].axhline(0, color=\"#AAAAAA\")\n",
    "    ax[0].plot(t, desired[:, 0], reflinestyle)\n",
    "    ax[0].plot(t, desired[:, 0] - threshold[:, 0], error_linestyle, alpha=0.5)\n",
    "    ax[0].plot(t, desired[:, 0] + threshold[:, 0], error_linestyle, alpha=0.5)\n",
    "\n",
    "    r = actual[:, 0]\n",
    "    ax[0].plot(t[:len(r)], r, linewidth=res_linewidth)\n",
    "\n",
    "    ax[0].grid(True)\n",
    "\n",
    "    \"\"\" PITCH \"\"\"\n",
    "\n",
    "    ax[1].axhline(0, color=\"#AAAAAA\")\n",
    "    ax[1].plot(t, desired[:, 1], reflinestyle)\n",
    "    ax[1].plot(t, desired[:, 1] - threshold[:, 1], error_linestyle, alpha=0.5)\n",
    "    ax[1].plot(t, desired[:, 1] + threshold[:, 1], error_linestyle, alpha=0.5)\n",
    "    p = actual[:, 1]\n",
    "    ax[1].plot(t[:len(p)], p, linewidth=res_linewidth)\n",
    "    ax[1].grid(True)\n",
    "\n",
    "    \"\"\" YAW \"\"\"\n",
    "    ax[2].axhline(0, color=\"#AAAAAA\")\n",
    "    ax[2].plot(t, desired[:, 2], reflinestyle)\n",
    "    ax[2].plot(t, desired[:, 2] - threshold[:, 2], error_linestyle, alpha=0.5)\n",
    "    ax[2].plot(t, desired[:, 2] + threshold[:, 2], error_linestyle, alpha=0.5)\n",
    "    y = actual[:, 2]\n",
    "    ax[2].plot(t[:len(y)], y, linewidth=res_linewidth)\n",
    "    ax[2].grid(True)\n",
    "\n",
    "    plt.savefig(\"gymfc-ppo-step-response.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index = random.randint(0, MAX_EPISODE-1)\n",
    "# print(desireds[index], actuals[index])\n",
    "plot_step_response(np.array(desireds), np.array(actuals))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
